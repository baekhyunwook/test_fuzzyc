Note: This testdata is from Linux kernel arch/x86/kernle/kvm.c with some
arbitrary modifications for testing the Fuzzyc (including this text).

#include <asm/desc.h>
#include <asm/tlbflush.h>
#include <asm/apic.h>
#include <asm/apicdef.h>
#include <asm/hypervisor.h>
#include <asm/tlb.h>

static int kvmapf = 1;

static int __init parse_no_kvmapf(char *arg)
{
        kvmapf = 0;
        return 0;
}

early_param("no-kvmapf", parse_no_kvmapf);

static int steal_acc = 1;
static int __init parse_no_stealacc(char *arg)
{
        steal_acc = 0;
        return 0;
}

early_param("no-steal-acc", parse_no_stealacc);

static DEFINE_PER_CPU_DECRYPTED(struct kvm_vcpu_pv_apf_data, apf_reason) __aligned(64);
static DEFINE_PER_CPU_DECRYPTED(struct kvm_steal_time, steal_time) __aligned(64);
static int has_steal_clock = 0;


...
...
...


/*
 * @interrupt_kernel: Is this called from a routine which interrupts the kernel
 *                    (other than user space)?
 */
void kvm_async_pf_task_wait(u32 token, int interrupt_kernel)
{
        u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);
        struct kvm_task_sleep_head *b = &async_pf_sleepers[key];
        struct kvm_task_sleep_node n, *e;
        DECLARE_SWAITQUEUE(wait);

        rcu_irq_enter();

        raw_spin_lock(&b->lock);
        e = _find_apf_task(b, token);
        if (e) {
                /* dummy entry exist -> wake up was delivered ahead of PF */
                hlist_del(&e->link);
                kfree(e);
                raw_spin_unlock(&b->lock);

                rcu_irq_exit();
                return;
        }

        n.token = token;
        n.cpu = smp_processor_id();
        n.halted = is_idle_task(current) ||
                   (IS_ENABLED(CONFIG_PREEMPT_COUNT)
                    ? preempt_count() > 1 || rcu_preempt_depth()
                    : interrupt_kernel);
        init_swait_queue_head(&n.wq);
        hlist_add_head(&n.link, &b->list);
        raw_spin_unlock(&b->lock);


        for (;;) {
                if (!n.halted)
                        prepare_to_swait_exclusive(&n.wq, &wait, TASK_UNINTERRUPTIBLE);
                if (hlist_unhashed(&n.link))
                        break;

                rcu_irq_exit();

                if (!n.halted) {
                        local_irq_enable();
                        schedule();
                        local_irq_disable();
                } else {
                        /*
                         * We cannot reschedule. So halt.
                         */
                        native_safe_halt();
                        local_irq_disable();
                }

                rcu_irq_enter();
        }
        if (!n.halted)
                finish_swait(&n.wq, &wait);

        rcu_irq_exit();
        return;
}
EXPORT_SYMBOL_GPL(kvm_async_pf_task_wait);

...


static void __init paravirt_ops_setup(void)
{
        pv_info.name = "KVM";

        if (kvm_para_has_feature(KVM_FEATURE_NOP_IO_DELAY))
                pv_ops.cpu.io_delay = kvm_io_delay;

#ifdef CONFIG_X86_IO_APIC
        no_timer_check = 1;
#endif
}

...

more code comes but not interested.